// Global default params, used in configs
params {
    //- Required workflow flags
    prefix = false
    basecalled_fastq = false

    //- Nanopolish flags
    nanopolish = false
    fast5_pass = false
    sequencing_summary = false

    //- Medaka flags
    medaka = false
    medakaModel = 'r941_min_hac_g507'
    flat = false // Input fastqs are only in 1 flat directory

    //- Scheme input params
    //-- Custom files I believe for Illumina which is decomissioned
    ref = false
    bed = false
    //-- Download scheme and input
    schemeRepoURL = 'https://github.com/phac-nml/primer-schemes.git'
    schemeDir = 'primer-schemes'
    scheme = 'nCoV-2019'
    schemeVersion = '2kb_resende'

    //- Nanopore options
    //-- Read count filtering
    minReadsArticGuppyPlex = 10
    minReadsPerBarcode = 100
    //-- Read length filtering - will be based on amplicon scheme and is done by artic guppyplex
    min_length = 1600
    max_length = 2400
    //-- Artic minion args
    normalise = 500
    bwa = false // Use bwa instead of minimap2
    noFrameshift = false // VCF filter `no-frameshift` arg checks for %3==0 allele calls

    //- Unneeded Illumina Args (as it is being removed here)
    illumina = false
    directory = false

    //- Config for running ncov-tools
    ncov = "$baseDir/extra_data/config.yaml"

    //- Metadata and IRIDA Uploads
    //-- Metadata is supplied with `--irida metadata.tsv` and requires specific columns. See README
    irida = false
    //-- Upload to IRIDA needs a config file along with the irida metadata TSV file. See README
    upload_irida = false

    //- Additional QC checks and files
    correctN = true
    pcr_primers = "$baseDir/extra_data/pcr_primers.bed"
    sequencingTechnology = 'Unknown'

    //- Typing module params
    //-- Typing frequency threshold to call aa consequences of variant. Set to ivarFreqThreshold for consistency with consensus
    csqAfThreshold = 0.75
    //-- Minimum coverage depth to call aa consequences of variant. Set to ivarMinDepth for consistency with consensus
    csqDpThreshold = 10

    //- Other boilerplate options
    outdir = './results'
    outCram = false
    help = false
    profile = false
    tracedir = "${params.outdir}/pipeline_info"

    //- cache option makes it a bit easier to set conda or singularity cacheDir
    cache = null

    //- Max resource options
    max_memory = '256.GB'
    max_cpus = 16
    max_time = '120.h'
}

// Load base.config by default for all pipelines
includeConfig 'conf/base.config'

// Load illumina specific config if running illumina
if ( params.illumina ){
    includeConfig 'conf/illumina.config'
}

profiles {
    conda {
        conda.enabled          = true
        conda.useMamba         = false
        includeConfig 'conf/conda.config'
        if ( params.cache ) {
            conda.cacheDir = params.cache
        }
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
    }
    mamba {
        conda.enabled          = true
        conda.useMamba         = true
        includeConfig 'conf/conda.config'
        if ( params.cache ) {
            conda.cacheDir = params.cache
        }
        docker.enabled         = false
        singularity.enabled    = false
        podman.enabled         = false
        shifter.enabled        = false
        charliecloud.enabled   = false
    }
    // No docker/singularity at the moment due to some steps
    slurm {
        process.executor = 'slurm'
    }
    lsf {
        process.executor = 'lsf'
    }
    //NML Canada Specific Config
    nml {
        includeConfig 'conf/nml.config'
    }
    // Test config to run tests on github
    test {
        includeConfig 'conf/test.config'
    }
}

// Capture exit codes from upstream processes when piping
process.shell = ['/bin/bash', '-euo', 'pipefail']

def timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "${params.tracedir}/execution_timeline_${timestamp}.html"
}
report {
    enabled = true
    file    = "${params.tracedir}/execution_report_${timestamp}.html"
}
trace {
    enabled = true
    file    = "${params.tracedir}/execution_trace_${timestamp}.txt"
}
dag {
    enabled = false
    file    = "${params.tracedir}/pipeline_dag_${timestamp}.html"
}

manifest {
    author          = 'Matt Bull, PHAC modifications from Darian Hole'
    description     = 'Nextflow for running the Artic ncov2019 pipeline'
    mainScript      = 'main.nf'
    nextflowVersion = '>=21.04.0'
    version         = '1.1.0'
    doi             = ''
    defaultBranch   = 'master'
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}
