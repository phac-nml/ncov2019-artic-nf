# ncov2019-artic-nf
A Nextflow pipeline for running the ARTIC network's fieldbioinformatics tools (https://github.com/artic-network/fieldbioinformatics), with a focus on nanopore ncov2019 

WARNING - THIS REPO IS UNDER ACTIVE DEVELOPMENT AND ITS BEHAVIOUR MAY CHANGE AT **ANY** TIME. 

PLEASE ENSURE THAT YOU READ BOTH THE README AND THE CONFIG FILE AND UNDERSTAND THE EFFECT OF THE OPTIONS ON YOUR DATA! 

### Introduction

------------

This Nextflow pipeline automates the ARTIC network [nCoV-2019 novel coronavirus bioinformatics protocol](https://artic.network/ncov-2019/ncov2019-bioinformatics-sop.html "nCoV-2019 novel coronavirus bioinformatics protocol"). It is being developed to aid the harmonisation of the analysis of sequencing data generated by the [COG-UK](https://github.com/COG-UK) project. It will turn SARS-COV2 sequencing data (Illumina or Nanopore) into consensus sequences and provide other helpful outputs to assist the project's sequencing centres with submitting data.

**This fork** adds a few changes and additions which have been made to optimize running this pipeline for NML Canada's *nanopore* infrastructure. Illumina data is currently not supported with the current implementation of this fork. Changes include:
- Renaming inputs using a samplesheet
- Bumping `artic` to v1.2.3
- Setting up outputs for upload to IRIDA
- Running [ncov-tools](https://github.com/jts/ncov-tools/blob/master/workflow/envs/environment.yml)
- More checks and additions to the final QC file
- Outputs files to show samples with no / too few reads that are filtered out

### Installation

------------

An up-to-date version of Nextflow is required because the pipeline is written in DSL2. 

You can either:
1. Download and install [conda](https://docs.conda.io/en/latest/miniconda.html) and then install Nextflow and Mamba using conda (Recommended)
    - Conda command: `conda create -n nextflow -c conda-forge -c bioconda nextflow mamba`
2. Following the instructions at https://www.nextflow.io/ to download and install Nextflow
    - You will still have to install other dependencies after

### Nanopore

------------

Note: We currently only support `conda` for dependency installation/containerization. It is recommended to have conda installed and use that when running this pipeline

#### **Method - Nanopolish**

*Running*

Basic command:
```
nextflow run phac-nml/ncov2019-artic-nf -profile conda --nanopolish --prefix "output_file_prefix" --basecalled_fastq /path/to/directory --fast5_pass /path/to/directory --sequencing_summary /path/to/sequencing_summary.txt
```

Command with new optional arguments:
```
nextflow run phac-nml/ncov2019-artic-nf [-profile conda,slurm,lsf,nml] --cache /path/to/conda_cache_dir/ --nanopolish --prefix "output_file_prefix" --basecalled_fastq /path/to/directory --fast5_pass /path/to/directory --sequencing_summary /path/to/sequencing_summary.txt --irida /path/to/samplesheet_with_names.tsv --upload_irida /path/to/irida_uploader/config
```

*IMPORTANT:* When running this pipeline it is important to specify your conda cache directory with `--cache 'path/to/cacheDir'` as generation of the ncov-tools env is extremly slow even with mamba. It should resolve but if not, you will have to install it yourself with `mamba env create -f=./ncov-tools/environment.yml -p path/to/cacheDir/ncovtools-<hash>`. The hash is from the nextflow and you will find it in the error for making the environment

------------

#### **Method - Medaka**

*Running*

Basic command:
```
nextflow run phac-nml/ncov2019-artic-nf -profile conda --medaka --prefix "output_file_prefix" --basecalled_fastq /path/to/directory --medakaModel "Model"
```

Command with new optional arguments:
```
nextflow run phac-nml/ncov2019-artic-nf [-profile conda,slurm,lsf,nml] --cache /path/to/conda_cache_dir/ --medaka --prefix "output_file_prefix" --basecalled_fastq /path/to/directory --medakaModel "Model" --irida /path/to/samplesheet_with_names.tsv --upload_irida /path/to/irida_uploader/config --flat
```

*IMPORTANT 1:* The medaka model should be set to the one that most closely matches your data. More info available here: https://github.com/nanoporetech/medaka#models

*IMPORTANT 2:* When running this pipeline it is important to specify your conda cache directory with `--cache 'path/to/cacheDir'` as generation of the ncov-tools env is extremly slow even with mamba. It should resolve but if not, you will have to install it yourself with `mamba env create -f=./ncov-tools/environment.yml -p path/to/cacheDir/ncovtools-<hash>`. The hash is from the nextflow and you will find it in the error for making the environment

#### **Nanopore Inputs**

#### --irida <samplesheet.tsv>

This argument will re-name all input samples to their respective sample name in the sample sheet based on the barcode. It will also allow ncov-tools to detect negative control samples and metadata to include that into the analysis.

To run the `--irida` argument, you must include a samplesheet in TSV format that contains the columns "sample", "run", "barcode", "project_id", "ct", and "date". They can be in any order as long as the first column is called "sample".

Below is an example samplesheet:

| sample | run | barcode | project_id | ct | date |
|-|-|-|-|-|-|
| Sample_name1 | run_name | 14 | 1422 | 23.33 | 2020-08-22 |
| Sample_name2 | run_name | 15 | 1422 | 22.53 | 2020-08-22 |
| Sample_name3 | run_name | 45 | 1422 | NA | NA |
| Sample_name6 | run_name | 65 | 1422 | NA | 2020-08-22 |

```
where:
    - sample     --> Contains the wanted sample name
    - run        --> Contains the wanted run name
    - barcode    --> Integer number that matches the sample name to the barcode
    - project_id --> Any number (if uploading to IRIDA put the IRIDA project), an NML specific need
    - ct         --> Ct number for ncov-tools (or NA if no ct number)
    - date       --> Sampling date for ncov-tools (yyyy-mm-dd) (NA if no date)
```
*Make sure there are no spaces in the sample names found in the samplesheet as it will cause an error to occur*

This will re-name all of the sample names based on the barcode so make sure that the barcodes match what you want the sample snames to be

Date and ct can be filled by `NA` if there is not one available

#### --upload_irida <irida_uploader.conf>

This argument will allow automatic upload of the raw inputs of the fast5 files + samplesheet and fastq files for each sample to the specified IRIDA instance in the config along with the generated consensus files (normal and corrected) and all metadata found in the final output

To run the upload you will need an IRIDA samplesheet and to use the `--irida` command along with the `--irida_upload` command. You will also need to give an IRIDA configuration that looks as such:

```
[Settings]
client_id = uploader
client_secret = <secret from IRIDA>
username = test
password = unsecure_password
base_url = https://<your_irida_instance>/irida/api
parser = directory
```

#### --correctN 

This argument is by default set to `true` and can be turned off by passing `--correctN false`. This argument will double check the failed variants file Ns to see if there sufficient evidence in the pileup to call a reference base at the location. This program was created as some spots were noted to fail but have a lot of evidence to have a reference base to call. Most notably are spots 16255-16256 and 24981-24982 which were found to have this issue in 80% of the output data.

This **does not** overwrite the nanopolish consensus file so both are output for comparison along with a log of what the correctN program has done.

#### Negative Controls

Negative controls can be used if `--irida` is given and one of the sample names has any of the following within it `negative`, `water`, `blank`, `ntc` in any capitalizations or spot.

This will run the negative control module of ncov-tools to check if there is any contamination in the run

*Negative Controls Limitations*

Connecting ncov-tools to the pipeline means that the ncov-tools config file has to be mostly set before running. In an attempt to make automation easier, the negative control names are set based on the key words mentioned (`negative`, `water`, `blank`, `ntc`) and passed by the samplesheet before. In the future, we may set-up a flag where you can set your negative control names but for now this is the easiest way to get all our users to run the pipeline.

---------------

### Executors
By default, the pipeline just runs on the local machine. You can specify `-profile slurm` to use a SLURM cluster, or `-profile lsf` to use an LSF cluster. It is recommended that you use your own config files though to allow better resource allocations

### Profiles
You can use multiple profiles at once, separating them with a comma. This is described in the Nextflow [documentation](https://www.nextflow.io/docs/latest/config.html#config-profiles) 

### Config
Common configuration options are set in `conf/base.config`. Workflow specific configuration options are set in `conf/nanopore.config` and `conf/illumina.config` They are described and set to sensible defaults (as suggested in the [nCoV-2019 novel coronavirus bioinformatics protocol](https://artic.network/ncov-2019/ncov2019-bioinformatics-sop.html "nCoV-2019 novel coronavirus bioinformatics protocol"))

---------------

### Illumina - Not Supported in this Fork
`nextflow run connor-lab/ncov2019-artic-nf [-profile conda,singularity,docker,slurm,lsf] --illumina --prefix "output_file_prefix" --directory /path/to/reads`

You can also use cram file input by passing the --cram flag.
You can also specify cram file output by passing the --outCram flag.

For production use at large scale, where you will run the workflow many times, you can avoid cloning the scheme repository, creating an ivar bed file and indexing the reference every time by supplying both --ivarBed /path/to/ivar-compatible.bed and --alignerRefPrefix /path/to/bwa-indexed/ref.fa.

Alternatively you can avoid just the cloning of the scheme repository to remain on a fixed revision of it over time by passing --schemeRepoURL /path/to/own/clone/of/github.com/artic-network/artic-ncov2019. This removes any internet access from the workflow except for the optional upload steps.

The Illumina workflow leans heavily on the excellent [ivar](https://github.com/andersen-lab/ivar) for primer trimming and consensus making. This workflow will be updated to follow ivar, as its also in very active development! Use `--illumina` to run the Illumina workflow. Use `--directory` to point to an Illumina output directory usually coded something like: `<date>_<machine_id>_<run_no>_<some_zeros>_<flowcell>`. The workflow will recursively grab all fastq files under this directory, so be sure that what you want is in there, and what you don't, isn't! 

Important config options are:

| Option | Description |
|:-------|------------:|
|allowNoprimer | Allow reads that don't have primer sequence? Ligation prep = false, nextera = true|
|illuminaKeepLen | Length of illumina reads to keep after primer trimming|
|illuminaQualThreshold | Sliding window quality threshold for keeping reads after primer trimming (illumina)|
|mpileupDepth | Mpileup depth for ivar|
|ivarFreqThreshold | ivar frequency threshold for variant|
|ivarMinDepth | Minimum coverage depth to call variant|
